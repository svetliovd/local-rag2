import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
from langchain_ollama import OllamaLLM as Ollama
from transformers import GenerationConfig
import os
import pymupdf4llm
import tempfile
import pandas as pd
import docx
import torch
import gc

# –ü–æ—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ meta tensor –ø—Ä–æ–±–ª–µ–º–∞
torch.classes.__path__ = []

CHROMA_DIR = "./chroma_db"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")

st.set_page_config(
    page_title="–í–∞—à–∏—è—Ç AI –∞—Å–∏—Å—Ç–µ–Ω—Ç",
    page_icon="./favicon.png"
)

# –°—Ç—Ä–∞–Ω–∏—á–Ω–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
language = st.sidebar.selectbox("–ò–∑–±–µ—Ä–∏ –µ–∑–∏–∫ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞:", ["–±—ä–ª–≥–∞—Ä—Å–∫–∏", "–∞–Ω–≥–ª–∏–π—Å–∫–∏"])

# –§—É–Ω–∫—Ü–∏–∏ –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∏ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞
@st.cache_resource(show_spinner=False)
def get_embedding_model():
    model_name = "BAAI/bge-m3"
    model_kwargs = {"device": "cpu"}
    encode_kwargs = {"normalize_embeddings": True}
    return HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)

@st.cache_resource(show_spinner=False)
def get_text_splitter():
    return SentenceTransformersTokenTextSplitter(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        chunk_size=100,
        chunk_overlap=20
    )

@st.cache_resource(show_spinner=False)
def get_llm():
    generation_params = {
        "max_new_tokens": 2048,
        "temperature": 0.1,
        "top_k": 25,
        "top_p": 1.0,
        "repetition_penalty": 1.1,
        "eos_token_id": [1, 107],
        "do_sample": True
    }

    return Ollama(
        base_url=ollama_host,
        model="todorov/bggpt:9B-IT-v1.0.Q8_0",
        generation_config=generation_params
    )

def clear_vector_store():
    import shutil
    import os

    # –ò–∑—Ç—Ä–∏–≤–∞–Ω–µ –Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ç–∞
    if os.path.exists(CHROMA_DIR):
        shutil.rmtree(CHROMA_DIR)
    # –ü—Ä–µ—Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –ø—Ä–∞–∑–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ø—Ä–∞–≤–∏–ª–Ω–∏ –ø—Ä–∞–≤–∞
    os.makedirs(CHROMA_DIR, exist_ok=True)
    # –ò–∑—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ –∫–µ—à–æ–≤–µ—Ç–µ

@st.cache_resource(show_spinner=False)
def get_vector_store():
    return Chroma(
        embedding_function=get_embedding_model(),
        persist_directory="./chroma_db",  # –ª–æ–∫–∞–ª–Ω–∞—Ç–∞ –ø–∞–ø–∫–∞
        collection_name="default"
    )

# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∏
@st.cache_data(show_spinner=False)
def load_documents(uploaded_files):
    text_splitter = get_text_splitter()
    extracted_pages = []
    documents = []
    for file in uploaded_files:
        if file.name.endswith(".pdf"):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
                tmp_pdf.write(file.read())
                tmp_pdf.flush()
                pdf_text = pymupdf4llm.to_markdown(tmp_pdf.name)
                documents.append(pdf_text)
                extracted_pages.append((file.name, [pdf_text]))
        elif file.name.endswith(".txt"):
            text = file.read().decode("utf-8")
            documents.append(text)
            extracted_pages.append((file.name, [text]))
        elif file.name.endswith(".csv"):
            df = pd.read_csv(file)
            text = df.to_string(index=False)
            documents.append(text)
            extracted_pages.append((file.name, [text]))
        elif file.name.endswith(".xlsx"):
            df = pd.read_excel(file, engine="openpyxl")
            text = df.to_string(index=False)
            documents.append(text)
            extracted_pages.append((file.name, [text]))
        elif file.name.endswith(".docx"):
            doc = docx.Document(file)
            text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            documents.append(text)
            extracted_pages.append((file.name, [text]))
        else:
            st.warning(f"–ù–µ–ø–æ–¥–¥—ä—Ä–∂–∞–Ω —Ç–∏–ø —Ñ–∞–π–ª: {file.name}")

    chunks = text_splitter.split_text("\n".join(documents))
    get_vector_store().add_texts(chunks)

    return extracted_pages

# CSS —Å—Ç–∏–ª–æ–≤–µ
custom_css = """
<style>
.stApp { background-color: #1C1C1C; }
.stApp, .stMarkdown, .stText, .stTextArea, .stRadio > label, .stButton > button {
    color: #F5E8D8;
}
.css-1v0mbdj, .css-17eq0hr, .css-1v3fvcr { color: #F5E8D8 !important; }
.stButton > button {
    background-color: #3E5641;
    color: #F5E8D8;
}
.stExpander, .stExpander div { color: #F5E8D8; }
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —á–∞—Ç –∏—Å—Ç–æ—Ä–∏—è—Ç–∞
if "history" not in st.session_state:
    st.session_state.history = []

# –û—Å–Ω–æ–≤–µ–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("–í–∞—à–∏—è—Ç AI –∞—Å–∏—Å—Ç–µ–Ω—Ç")

# –ö–∞—á–≤–∞–Ω–µ –Ω–∞ —Ñ–∞–π–ª–æ–≤–µ
uploaded_files = st.file_uploader(
    "–ö–∞—á–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ (PDF, TXT, XLSX, CSV, DOCX)",
    accept_multiple_files=True,
    type=["txt", "pdf", "xlsx", "csv", "docx"]
)

if uploaded_files:
    extracted_pages = load_documents(uploaded_files)
    st.success("–î–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ!")
    st.subheader("üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ:")
    for filename, pages in extracted_pages:
        with st.expander(f"{filename} ({len(pages)} —Å—Ç—Ä.)"):
            tab_names = [f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i + 1}" for i in range(len(pages))]
            tabs = st.tabs(tab_names)
            for tab, page in zip(tabs, pages):
                with tab:
                    page_content = page if isinstance(page, str) else page.page_content
                    st.write(page_content)

# –°–µ–∫—Ü–∏—è –∑–∞ —á–∞—Ç –∏—Å—Ç–æ—Ä–∏—è—Ç–∞
st.subheader("üó®Ô∏è –ò—Å—Ç–æ—Ä–∏—è –Ω–∞ —á–∞—Ç–∞")

# –ë—É—Ç–æ–Ω –∑–∞ –∏–∑—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ —á–∞—Ç –∏—Å—Ç–æ—Ä–∏—è—Ç–∞
if st.button("üóëÔ∏è –ò–∑—á–∏—Å—Ç–∏ –∏—Å—Ç–æ—Ä–∏—è—Ç–∞ –Ω–∞ —á–∞—Ç–∞", key="clear_chat_history"):
    st.session_state.clear()  # Clears all session data including history
    st.rerun()  # Immediately refreshes the interface
    
# –ë—É—Ç–æ–Ω –∑–∞ –∏–∑—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ –≤–µ–∫—Ç–æ—Ä–Ω–∞—Ç–∞ –±–∞–∑–∞
with st.sidebar.expander("üßπ –ò–∑—á–∏—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω–∞—Ç–∞ –±–∞–∑–∞"):
    st.warning("‚ö†Ô∏è –¢–æ–≤–∞ —â–µ –∏–∑—Ç—Ä–∏–µ –≤—Å–∏—á–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –æ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–∞—Ç–∞ –±–∞–∑–∞ –∏ –Ω–µ –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –≤—ä—Ä–Ω–∞—Ç–æ –æ–±—Ä–∞—Ç–Ω–æ.")
    if st.sidebar.button("‚úÖ –ü–æ—Ç–≤—ä—Ä–¥–∏ –∏–∑—á–∏—Å—Ç–≤–∞–Ω–µ", key="confirm_clear_vector_db"):
        clear_vector_store()
        st.success("–í–µ–∫—Ç–æ—Ä–Ω–∞—Ç–∞ –±–∞–∑–∞ –µ –∏–∑—á–∏—Å—Ç–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        st.rerun()

# –ü–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ —á–∞—Ç –∏—Å—Ç–æ—Ä–∏—è—Ç–∞
chat_container = st.container()
with chat_container:
    if st.session_state.history:
        for i, (question, answer) in enumerate(st.session_state.history):
            with st.expander(f"–í—ä–ø—Ä–æ—Å {i + 1}: {question[:50]}..."):
                st.markdown(f"**–í—ä–ø—Ä–æ—Å:** {question}")
                st.markdown(f"**–û—Ç–≥–æ–≤–æ—Ä:** {answer}")
    else:
        st.info("–ù—è–º–∞ –∑–∞–ø–∞–∑–µ–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è –Ω–∞ —á–∞—Ç–∞.")

# –§–æ—Ä–º–∞ –∑–∞ –≤—ä–ø—Ä–æ—Å–∏
st.subheader("–ó–∞–¥–∞–π –≤—ä–ø—Ä–æ—Å")
with st.form(key="query_form", clear_on_submit=True):
    query = st.text_area("–í—ä–≤–µ–¥–∏ —Ç–≤–æ—è –≤—ä–ø—Ä–æ—Å —Ç—É–∫:", height=150, key="query_input")
    submit_button = st.form_submit_button("–ü–æ–ø–∏—Ç–∞–π")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –≤—ä–ø—Ä–æ—Å–∞
if submit_button and query:
    docs = get_vector_store().similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    chat_history = ""
    for i, (past_query, past_response) in enumerate(st.session_state.history[-5:]):
        chat_history += f"–í—ä–ø—Ä–æ—Å {i+1}: {past_query}\n–û—Ç–≥–æ–≤–æ—Ä {i+1}: {past_response}\n\n"

    lang_text = "–±—ä–ª–≥–∞—Ä—Å–∫–∏" if language == "–±—ä–ª–≥–∞—Ä—Å–∫–∏" else "–∞–Ω–≥–ª–∏–π—Å–∫–∏"

    prompt = (
        f"–ò—Å—Ç–æ—Ä–∏—è –Ω–∞ —á–∞—Ç–∞:\n{chat_history}\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∏: {context}\n"
        f"–ù–æ–≤ –≤—ä–ø—Ä–æ—Å: {query}\n"
        f"–û—Ç–≥–æ–≤–æ—Ä –Ω–∞ {lang_text}:"
    )

    llm = get_llm()
    st.write("**–û—Ç–≥–æ–≤–æ—Ä:**")
    response_container = st.empty()
    full_response = ""

    try:
        for chunk in llm.stream(prompt):
            full_response += chunk
            response_container.markdown(full_response)
    except AttributeError:
        response = llm.invoke(prompt)
        full_response = response
        response_container.markdown(full_response)

    st.session_state.history.append((query, full_response))
    gc.collect()
